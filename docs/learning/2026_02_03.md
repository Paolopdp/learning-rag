# Informazioni di contesto

# Cos'ho fatto fino ad ora

Ho costruito un “primo slice” (MVP minimale) del backend di un sistema RAG:

1. Carica alcuni documenti di esempio, li spezza in pezzi (“chunk”), calcola vettori numerici
   (“embedding”) e poi, data una domanda, recupera i pezzi più simili e li restituisce con “citazioni” (in realtà riferimenti alla fonte del chunk).

Si tratta di un servizio web Python basato su FastAPI (app/main.py) con alcune API HTTP.

- Implementa una pipeline RAG “minima”:
  - Ingestione: leggere documenti → divisione in chunk → embedding → salvataggio.
  - Recupero del dato: embedding della domanda → confronto con embedding dei chunk → top-k risultati.
  - Risposta: per ora non usa un LLM; restituisce come “answer” il testo del chunk migliore.

# Dipendenze e setup progetto

- Configurazione pacchetto in pyproject.toml.
- Python richiesto: >=3.13.
- Librerie principali:
  - fastapi + uvicorn[standard]: server web e runtime ASGI.
  - numpy: calcoli numerici (vettori/matrici).
  - sentence-transformers: modello pre-addestrato per creare embedding.
- Test: pytest (+ httpx per richieste in test).

# Dati demo

- I file demo sono contenuti nella cartella del repo: ../data/wikipedia_it/\*.txt.
- La path viene calcolata in app/config.py:
  - repo_root() risale di 2 cartelle da backend/app/config.py fino alla root del repo
    (cioè .../RAG/).
  - wikipedia_it_dir() punta a .../RAG/data/wikipedia_it.
- I file .txt hanno un piccolo “header” iniziale (metadati) e poi il corpo del testo.
  Esempio: ../data/wikipedia_it/spid.txt.
- C’è anche documentazione sul dataset:
  - ../data/ATTRIBUTION.md spiega fonti e licenze (Wikipedia IT, CC BY-SA ecc.).
  - ../data/golden_queries.md contiene 3 domande “golden” per provare il sistema e sapere
    quali file dovrebbero essere citati.

# Modelli dati: Document e Chunk

Definiti in app/models.py usando @dataclass (è un modo semplice per creare “strutture dati” immutabili e leggibili):

- Document: rappresenta un documento intero (titolo, URL fonte, licenza, data accesso, testo, e
  un document_id uuid).
- Chunk: rappresenta un pezzo di documento:
  - content: testo del chunk.
  - start_char, end_char: intervallo di caratteri nel testo normalizzato.
  - chunk_index: ordine del chunk dentro il documento.
  - source_title, source_url: per la “citazione”.
  - chunk_id uuid.

# Ingestione

Viene fatta in in app/ingestion.py.

Parsing file

- load_documents_from_dir(directory): prende tutti i \*.txt e chiama parse_wikipedia_file.
- parse_wikipedia_file(path):
  - legge il file,
  - separa header e body con \_split_header_body (split su prima riga vuota),
  - interpreta l’header con \_parse_header.
- \_parse_header mappa chiavi italiane a campi “interni”:
  - Titolo: → title
  - Fonte: → source_url
  - Licenza: → license
  - Accesso: → accessed_at (parse ISO, es. 2026-02-03)

Chunking (spezzare il testo)

- chunk_document(document, chunk_size=600, overlap=120):
  - normalizza testo con \_normalize_text (collassa spazi e newlines in spazi singoli: return
    " ".join(text.split())).
  - crea finestre di lunghezza circa chunk_size.
  - usa un “trucco” per non tagliare parole a metà: cerca l’ultimo spazio prima del limite
    (rfind(" ", start, end)).
  - usa overlap: il chunk successivo riparte un po’ prima della fine del precedente (end -
    overlap), per non perdere contesto quando una frase attraversa un confine.
- chunk_documents(...) fa lo stesso su una lista di documenti.

# Embeddings: trasformare testo in vettori

In app/embeddings.py:

- Un embedding è un vettore di numeri (es. 384 dimensioni, dipende dal modello) che rappresenta
  il significato del testo in modo “geometrico”.
- Modello default: intfloat/multilingual-e5-small.
- Puoi cambiarlo via variabile d’ambiente: RAG_EMBEDDING_MODEL.
- get_embedder è cacheato con lru_cache(maxsize=1):
  - significa che il modello viene caricato una sola volta e riusato
- embed_texts(texts) produce una matrice np.ndarray (una riga per testo).
- embed_text(text) produce un singolo vettore.
- normalize_embeddings=True: normalizza i vettori (utile per la cosine similarity).

# Store

In app/store.py:

- InMemoryChunkStore salva tutto in RAM (quindi se riavvii il server, perdi i dati).
- Tiene:
  - chunks: list[Chunk]
  - embeddings: list[list[float]] (salvati come liste “pure” per semplicità; quando serve si
    riconverte in np.array).
- add_many controlla che len(chunks) == len(embeddings) (coerenza dati).
- embedding_matrix() restituisce la matrice np.ndarray per fare calcoli veloci.

# Retrieval

In app/retrieval.py:

- Concetto: abbiamo embedding dei chunk (matrice) e embedding della domanda (vettore). Vogliamo un
  punteggio di somiglianza per ogni chunk.
- cosine_similarity_scores(matrix, vector) calcola:
  - score = (matrix @ vector) / (||matrix|| \* ||vector||)
  - dove @ è prodotto scalare/matriciale, ||...|| è norma (lunghezza del vettore).
- top_k_chunks(...):
  - calcola gli score per tutti i chunk,
  - ordina dal più alto al più basso,
  - restituisce i migliori k come RetrievalResult(chunk, score).

# API FastAPI: cosa puoi chiamare

Definite in app/main.py:

- GET /health
  - serve per capire se il server è vivo: ritorna {"status":"ok"}.
- POST /ingest/demo
  - carica i documenti da ../data/wikipedia_it,
  - li divide in chunk,
  - calcola embedding di ogni chunk,
  - svuota lo store e inserisce i nuovi dati.
  - ritorna conteggi: numero documenti e numero chunk.
- GET /chunks?limit=5
  - mostra alcuni chunk salvati, inclusi source_title e source_url (utile per vedere cosa è
    stato indicizzato).
- POST /query
  - input: question e top_k (1–10).
  - se non hai ingestito: errore 400 “No data ingested yet.”
  - altrimenti:
    1. embedding della domanda,
    2. top-k chunk simili,
    3. risposta:
       - answer: per ora è il testo del chunk migliore (non è “generato”, è estratto).
       - citations: lista con chunk_id, titolo/url sorgente, score, ed excerpt (prime 200
         char del chunk).

# Test

In tests/:

- tests/test_health.py: che /health risponda 200 e JSON atteso.
- tests/test_ingestion.py:
  - parsing di un file con header (inclusa data 2026-02-03),
  - chunking: che produca più di un chunk e che gli indici siano coerenti.
- tests/test_retrieval.py:
  - dato un caso semplice con embedding “finti”, verifica che l’ordinamento top-k sia
    corretto.

Cosa manca (e perché è ok in questa fase)

- Non c’è ancora: database (Postgres/pgvector), autenticazione/ruoli, audit log, observability,
  UI, LLM per risposta “grounded”.
- Questo è volutamente un prototipo didattico: dimostra la pipeline base end-to-end e
  permette di fare il passo successivo con più sicurezza.

# Chiarimenti

Pensa agli embedding come a un modo per trasformare il testo in numeri.

1. Perché servono i vettori

- Un computer confronta bene i numeri.
- Confronta male direttamente le frasi, perché due frasi possono dire la stessa cosa con parole
  diverse.

Esempio:

- “SPID serve per accedere ai servizi online”
- “Con SPID entri nei servizi digitali della PA”
  Sono simili come significato, ma le parole cambiano.

2. Cos’è un vettore (senza matematica pesante)

Un vettore è una lista ordinata di numeri, tipo:

- [0.12, -0.03, 0.88, ...]

Puoi immaginarlo come una “posizione” in uno spazio con tante dimensioni (non 2D o 3D, ma
magari 384 dimensioni). Non devi visualizzarlo: ti basta sapere che:

- testi simili → vettori “vicini”
- testi diversi → vettori “lontani”

3. Cos’è un embedding

Un embedding è proprio quel vettore che rappresenta il testo.

In app/embeddings.py:

- SentenceTransformer(model).encode(text) prende una frase e ti dà il suo embedding.
- normalize_embeddings=True fa una normalizzazione: in pratica rende i vettori “comparabili in modo pulito” (li mette su una scala standard).

3. Cosa succede durante l’ingestione

Quando fai POST /ingest/demo (app/main.py):

1. Leggi i documenti.
2. Li spezzi in chunk (pezzi piccoli).
3. Per ogni chunk calcoli un embedding.
4. Salvi:
   - la lista dei chunk (testo + metadati)
   - la lista dei loro embedding (numeri)

Quindi ogni chunk ha “due versioni”:

- versione umana: testo
- versione per macchina: vettore di numeri

5. Cosa succede quando fai una domanda
   Quando fai POST /query:

1. Prendi la tua domanda (testo).
1. Calcoli l’embedding della domanda (un vettore).
1. Confronti questo vettore con tutti i vettori dei chunk.
1. Prendi i chunk più simili (top-k).

1. Come fa a dire “simili” o “non simili”
   Qui entra la “cosine similarity” (app/retrieval.py).

Senza formule: immagina che ogni vettore sia una freccia.

- Se due frecce puntano nella stessa direzione → i testi sono simili → punteggio alto.
- Se puntano in direzioni diverse → testi diversi → punteggio basso.

Nel codice:

- cosine_similarity_scores(embeddings_matrix, query_vector) produce un punteggio per ogni chunk.
- Poi top_k_chunks(...) ordina i punteggi e prende i migliori.

7. Perché non basta cercare parole uguali

Se cercassi solo parole, avresti problemi:

- sinonimi: “accesso” vs “login”
- forme diverse: “servizi online” vs “servizi digitali”
- frasi più lunghe o più corte

Gli embedding aiutano perché catturano somiglianza “di significato” (approssimata).

8. Collegamento diretto ai file del progetto

- Creazione embedding: app/embeddings.py
- Similarità e top-k: app/retrieval.py
- Uso nella query: app/main.py nella funzione query()
